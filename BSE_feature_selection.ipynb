{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import RFE,RFECV,SelectKBest\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cycifsuite.get_data import read_synapse_file\n",
    "from rfpimp import importances\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection based on sampling and emsembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(path, fname, ligand_name=None, sample_size=None, excluding_cells=[]):\n",
    "    '''Read data and construct\n",
    "    '''\n",
    "    os.chdir(path)\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    x = pd.read_hdf(fname)\n",
    "    zero_std_cols = x.columns[x.std() == 0]\n",
    "    x = x.drop(zero_std_cols, axis=1)\n",
    "    x.loc[:, :] = imputer.fit_transform(x)\n",
    "\n",
    "    # Sampling easier feature selection\n",
    "    if sample_size is not None:\n",
    "        if isinstance(sample_size, float):\n",
    "            sample_size = int(sample_size * x.shape[0])\n",
    "        elif isinstance(sample_size, int):\n",
    "            pass\n",
    "        # get rid of cells not wanted\n",
    "        sample_pool = [k for k in x.index if k not in excluding_cells]\n",
    "        sample_idx = np.random.choice(\n",
    "            x.index, size=sample_size)\n",
    "        x = x.loc[sample_idx]\n",
    "    # make y_vector\n",
    "    y_vector = None\n",
    "    if ligand_name is not None:\n",
    "        y_vector = [ligand_name] * x.shape[0]\n",
    "    return x, y_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome, Yunguan Wang!\n",
      "\n",
      "['plate_6_txt_features_BMP2.hdf', 'plate_6_txt_features_EGF.hdf', 'plate_6_txt_features_HGF.hdf', 'plate_6_txt_features_IFNG.hdf', 'plate_6_txt_features_OSM.hdf', 'plate_6_txt_features_PBS.hdf', 'plate_6_txt_features_TGFB.hdf']\n",
      "Processing BMP2\n",
      "Processing EGF\n",
      "Processing HGF\n",
      "Processing IFNG\n",
      "Processing OSM\n",
      "Processing PBS\n",
      "Processing TGFB\n"
     ]
    }
   ],
   "source": [
    "path = 'd:/data/MCF10A 090718 data/'\n",
    "os.chdir(path)\n",
    "feature_files = [x for x in os.listdir() if ('plate_6_txt_' in x) and 'all' not in x]\n",
    "pooled_metadata = pd.read_csv(read_synapse_file('syn17902177'),index_col=0)\n",
    "print(feature_files)\n",
    "x = pd.DataFrame()\n",
    "y = []\n",
    "for fn in feature_files:\n",
    "    ligand = fn.split('_')[-1][:-4]\n",
    "    print('Processing {}'.format(ligand))\n",
    "    _x, _y = data_preprocessing(path,fn,ligand_name=ligand, sample_size=1500)\n",
    "    if x.shape[0]==0:\n",
    "        x = _x\n",
    "    else:\n",
    "        cols = [k for k in x.columns if k in _x.columns]\n",
    "        x = x[cols].append(_x[cols])\n",
    "    y+=_y\n",
    "x.loc[:,:] = minmax_scale(x)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.20)\n",
    "num_features = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get feature importance based on Anova in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] C:\\WinPython\\python-3.6.5.amd64\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [562] are constant.\n",
      "  UserWarning)\n",
      "\n",
      "[WARNING] C:\\WinPython\\python-3.6.5.amd64\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Anova\n",
    "sk = SelectKBest(k=num_features)\n",
    "sk.fit(x_train,y_train)\n",
    "sk_fs = sk.get_support()\n",
    "sk_fs = x.columns[sk_fs].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get feature importance and top features on classical RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8400, 5046)\n",
      "Full feature accuracy 0.9514285714285714\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "full_model = RandomForestClassifier(100)\n",
    "full_model.fit(x_train, y_train)\n",
    "y_pred = full_model.predict(x_test)\n",
    "print('Full feature accuracy {}'.format(acc(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_full = pd.Series(full_model.feature_importances_, index=x.columns)\n",
    "rf_fs = fi_full.sort_values(ascending=False).index[:num_features].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get feature importance and top features based on cross-validated RF feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full feature accuracy 0.9619047619047619\n"
     ]
    }
   ],
   "source": [
    "rfcv_x_train, rfcv_x_test, rfcv_y_train, rfcv_y_test = train_test_split(x_train,y_train, test_size=0.20)\n",
    "rfcv = RandomForestClassifier(100, n_jobs=4)\n",
    "rfcv.fit(rfcv_x_train,rfcv_y_train)\n",
    "rfcv_y_pred = rfcv.predict(rfcv_x_test)\n",
    "print('Full feature accuracy {}'.format(acc(rfcv_y_test, rfcv_y_pred)))\n",
    "fs_imp = importances(rfcv, rfcv_x_test, pd.Series(rfcv_y_test)) # permutation\n",
    "rfpimp_fs = fs_imp.index[:num_features].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get top features using RFE with random forest for non-linear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 400 features.\n",
      "Fitting estimator with 375 features.\n",
      "Fitting estimator with 350 features.\n",
      "Fitting estimator with 325 features.\n",
      "Fitting estimator with 300 features.\n",
      "Fitting estimator with 275 features.\n",
      "Fitting estimator with 250 features.\n",
      "Fitting estimator with 225 features.\n",
      "Fitting estimator with 200 features.\n",
      "Fitting estimator with 175 features.\n",
      "Fitting estimator with 150 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 75 features.\n"
     ]
    }
   ],
   "source": [
    "rfe_input_features = fi_full.sort_values(ascending=False).index[:500]\n",
    "sampled_x = x_train[rfe_input_features]\n",
    "sampled_y = y_train\n",
    "estimator = RandomForestClassifier(100)\n",
    "selector = RFE(estimator, n_features_to_select=num_features, step=25, verbose=1)\n",
    "selector = selector.fit(sampled_x, sampled_y)\n",
    "rfe_fs_rf = sampled_x.columns[selector.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get top features using RFE with logistic regression for linear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 500 features.\n",
      "Fitting estimator with 475 features.\n",
      "Fitting estimator with 450 features.\n",
      "Fitting estimator with 425 features.\n",
      "Fitting estimator with 400 features.\n",
      "Fitting estimator with 375 features.\n",
      "Fitting estimator with 350 features.\n",
      "Fitting estimator with 325 features.\n",
      "Fitting estimator with 300 features.\n",
      "Fitting estimator with 275 features.\n",
      "Fitting estimator with 250 features.\n",
      "Fitting estimator with 225 features.\n",
      "Fitting estimator with 200 features.\n",
      "Fitting estimator with 175 features.\n",
      "Fitting estimator with 150 features.\n",
      "Fitting estimator with 125 features.\n",
      "Fitting estimator with 100 features.\n",
      "Fitting estimator with 75 features.\n"
     ]
    }
   ],
   "source": [
    "rfe_input_features = fi_full.sort_values(ascending=False).index[:500]\n",
    "sampled_x = x_train[rfe_input_features]\n",
    "sampled_y = y_train\n",
    "estimator = LogisticRegression(solver='newton-cg', multi_class='auto')\n",
    "selector = RFE(estimator, n_features_to_select=num_features, step=25, verbose=1)\n",
    "selector = selector.fit(sampled_x, sampled_y)\n",
    "rfe_fs_lr = sampled_x.columns[selector.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summerize over all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pd.DataFrame(index = x_train.columns)\n",
    "for col, fs_list in zip(['Anova','RF','RF_cv','RFE_rf','RFE_lr'],[sk_fs, rf_fs, rfpimp_fs, rfe_fs_rf, rfe_fs_lr]):\n",
    "    all_features.loc[fs_list,col] = 1\n",
    "all_features.fillna(0,inplace=True)\n",
    "all_features = all_features.sum(axis=1).sort_values(ascending=False)\n",
    "all_features = pd.DataFrame(all_features, columns=['feature_rank'])\n",
    "all_features['RF_cv_fi'] = fs_imp.loc[all_features.index].values\n",
    "all_features['RF_fi'] = fi_full.loc[all_features.index].values\n",
    "all_features = all_features.sort_values(['feature_rank','RF_cv_fi','RF_fi'], ascending=False)\n",
    "best_features = all_features.index.tolist()[:num_features]\n",
    "three_ab = all_features[all_features.feature_rank>=3].index.tolist()\n",
    "four_ab = all_features[all_features.feature_rank>=4].index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate features on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anova: Selected 50 feature accuracy 0.91\n",
      "RF: Selected 50 feature accuracy 0.95\n",
      "RF_cv: Selected 50 feature accuracy 0.95\n",
      "RFE_rf: Selected 50 feature accuracy 0.96\n",
      "RFE_lr: Selected 50 feature accuracy 0.96\n",
      "Best: Selected 50 feature accuracy 0.95\n",
      "Shared >=3: Selected 39 feature accuracy 0.95\n",
      "Shared >=4: Selected 18 feature accuracy 0.93\n",
      "Full feature model accuracy 0.95\n"
     ]
    }
   ],
   "source": [
    "for col, fs_list in zip(['Anova','RF','RF_cv','RFE_rf','RFE_lr','Best', 'Shared >=3', 'Shared >=4'],[sk_fs, rf_fs, rfpimp_fs, rfe_fs_rf, rfe_fs_lr, best_features, three_ab, four_ab]):\n",
    "    sub_features = fs_list\n",
    "    x_train_sub = x_train[sub_features]\n",
    "    x_test_sub = x_test[sub_features]\n",
    "    fs_model = RandomForestClassifier(100)\n",
    "    fs_model.fit(x_train_sub, y_train)\n",
    "    y_pred = fs_model.predict(x_test_sub)\n",
    "    print('{}: Selected {} feature accuracy {:.2f}'.format(col,len(sub_features),acc(y_test, y_pred)))\n",
    "y_pred = full_model.predict(x_test)\n",
    "print('Full feature model accuracy {:.2f}'.format(acc(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-2 test pval: 0.000, abnormal class distribution in  best features.\n",
      "Enrichment of int features observed\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chisquare as chi2\n",
    "f_ob = len([k for k in best_features if '_int_' in k])\n",
    "f_ob = [f_ob, len(best_features)-f_ob]\n",
    "f_exp = len([k for k in x.columns if '_int_' in k])\n",
    "f_exp = [f_exp, x.shape[1]-f_exp]\n",
    "pval = chi2(f_ob, f_exp)[1]\n",
    "if pval >= 0.05:\n",
    "    print('Chi-2 test pval: {:.3f}, no enrichment of either class in the best features.'.format(pval))\n",
    "else:\n",
    "    print('Chi-2 test pval: {:.3f}, abnormal class distribution in  best features.'.format(pval))\n",
    "    if f_ob[0]/sum(f_ob)>f_exp[0]/sum(f_exp):\n",
    "        print('Enrichment of int features observed')\n",
    "    else:\n",
    "        print('Enrichment of txt features observed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('d:/data/Best_features.txt', 'a') as f:\n",
    "    f.write(', '.join(best_features))\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation based estimate on feature set significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0\n",
    "# x_train_sub = x_train[best_features]\n",
    "# x_test_sub = x_test[best_features]\n",
    "# fs_model = RandomForestClassifier(100)\n",
    "# fs_model.fit(x_train_sub, y_train)\n",
    "# y_pred = fs_model.predict(x_test_sub)\n",
    "# best_fs_acc = acc(y_test, y_pred)\n",
    "# acc_best_inferior = 0\n",
    "# print('Selected best feature accuracy {:.2f}'.format(acc(y_test, y_pred)))\n",
    "# while i < 100:\n",
    "#     random_features = np.random.choice(x.columns, replace=False, size=num_features)\n",
    "#     test_model = RandomForestClassifier(100)\n",
    "#     fs_model.fit(x_train[random_features], y_train)\n",
    "#     y_pred = fs_model.predict(x_test[random_features])\n",
    "#     random_fs_acc = acc(y_test, y_pred)\n",
    "#     if random_fs_acc>=best_fs_acc:\n",
    "#         print('Randomly selected feature accuracy: {:.2f}'.format(random_fs_acc))\n",
    "#         acc_best_inferior+=1\n",
    "#     i+=1\n",
    "# print('Pval of best feature : {}'.format(acc_best_inferior/100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
